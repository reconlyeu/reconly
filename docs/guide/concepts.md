# Concepts

Reconly turns raw content from across the web into structured, searchable intelligence. This page explains how the core pieces fit together.

## Entity Flow

```
Sources  →  Feeds  →  Feed Runs  →  Digests  →  Export
```

**Sources** define *what* to monitor — an RSS feed, a YouTube channel, a website, an email inbox, or a research topic for an AI agent.

**Feeds** group one or more sources together and define *when* and *how* to process them. A feed has a schedule (cron), a digest mode, and a prompt template that controls summarization style.

**Feed Runs** are individual executions of a feed. Each run fetches new content from the feed's sources, summarizes it with your configured LLM, and produces a digest.

**Digests** are the output — summarized articles ready to read, search, tag, and export. Each digest entry links back to its original source content.

**Export** sends digests elsewhere: email newsletters, webhooks for automation, or PKM sync to Obsidian/Logseq.

## Sources

A source is a content endpoint. Reconly supports several source types:

| Type | What It Monitors |
|------|-----------------|
| RSS / Atom | Standard web feeds |
| YouTube | Channel or playlist videos |
| Website | Arbitrary web pages (scraped) |
| IMAP Email | Email inbox messages |
| Agent Research | AI-driven topic investigation |

Each source tracks a **circuit breaker** — if fetching fails repeatedly, the source is temporarily disabled to avoid wasting resources. You can reset it manually when the issue is resolved.

## Feeds

A feed ties sources to a processing pipeline:

- **Sources** — which sources to include
- **Schedule** — when to run (cron expression, e.g. daily at 8 AM)
- **Digest Mode** — how to combine content:
  - *Individual*: one digest entry per article
  - *Combined*: all articles merged into one summary
  - *Grouped*: articles grouped by source
- **Prompt Template** — controls the LLM summarization style and language
- **Report Template** — controls output formatting (Markdown, HTML, JSON)
- **Filters** — keyword include/exclude rules applied before summarization

## Feed Runs

Every time a feed's schedule fires (or you trigger it manually), a feed run is created. The run:

1. Fetches new content from each source
2. Applies content filters
3. Sends content to the LLM for summarization
4. Generates embeddings for semantic search (if RAG is enabled)
5. Extracts entities for the knowledge graph (if enabled)
6. Produces digest entries
7. Triggers any configured exports (email, webhooks, PKM)

You can view the run history to see what was fetched, how long it took, and whether any errors occurred.

## Digests

Digests are the primary output. Each digest entry contains:

- **Title** — from the original article or generated by the LLM
- **Summary** — LLM-generated summary based on your prompt template
- **Source link** — URL to the original content
- **Tags** — auto-generated or manually added
- **Metadata** — publish date, source name, feed name

You can browse digests in card or table view, filter by feed/source/tag/date, and export individual items or bulk selections.

## Templates

Templates control how content is processed and formatted:

- **Prompt Templates** — instructions for the LLM (language, length, style, focus areas)
- **Report Templates** — output format (Markdown, HTML, JSON) with layout control

Templates can be **built-in** (shipped with Reconly), **user-created**, or **imported** from feed bundles.

## Chat & RAG

When RAG (Retrieval-Augmented Generation) is enabled, all digest content is embedded into a vector database. The chat interface lets you ask questions across your entire archive with cited sources.

Chat supports:
- Quick access via `Ctrl+K`
- Full-page chat for longer conversations
- Automatic citation of relevant digest entries

## Knowledge Graph

The knowledge graph visualizes connections between entities (people, organizations, topics, technologies) extracted from your digests. Navigate the graph to discover relationships you might not have noticed from reading individual articles.

## AI Providers

Reconly works with multiple LLM providers for summarization:

| Provider | Privacy | Cost |
|----------|---------|------|
| Ollama / LM Studio | 100% local | Free |
| HuggingFace | Cloud | Free tier |
| OpenAI | Cloud | Pay per use |
| Anthropic | Cloud | Pay per use |

You can configure different providers per feed or use a global default.
