# Reconly OSS - Docker Environment Configuration
# Copy this file to .env and configure your settings

# =============================================================================
# Quick Start
# =============================================================================

# Load sample data on first start
# Set to false for clean start with wizard, true to explore with demo content
LOAD_SAMPLE_DATA=false

# Server port
API_PORT=8000

# =============================================================================
# LLM Provider - Ollama (recommended for local/OSS use)
# =============================================================================
#
# PREREQUISITES - Run these commands BEFORE starting Reconly:
#   1. Install Ollama: https://ollama.com/download
#   2. Pull a model:   ollama pull qwen2.5:7b
#
# NOTE: qwen2.5:7b works well for feed summarization, but AI Research Agents
# in comprehensive/deep mode need a larger model (32B+ recommended) to
# synthesize search results accurately. Small models tend to hallucinate.
# For research agents, either use a larger local model (e.g., qwen2.5:32b)
# or a cloud provider (Anthropic, OpenAI) below.
#
DEFAULT_PROVIDER=ollama
# Change this to any model you have (e.g., qwen2.5:14b, llama3.2, mistral)
OLLAMA_MODEL=qwen2.5:7b
OLLAMA_HOST=http://host.docker.internal:11434

# =============================================================================
# Alternative LLM Providers (cloud-based)
# =============================================================================

# Anthropic Claude
# ANTHROPIC_API_KEY=sk-ant-...

# OpenAI
# OPENAI_API_KEY=sk-...

# HuggingFace
# HUGGINGFACE_API_KEY=hf_...

# =============================================================================
# Edition & Security
# =============================================================================

# Edition (oss or enterprise)
RECONLY_EDITION=oss

# Optional: Password protection for the UI
# Leave empty for no authentication
# RECONLY_AUTH_PASSWORD=your-secret-password
